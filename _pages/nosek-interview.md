
*event description*

**[BC] We found that lots of students had concerns and were interested in this kind of thing, they just didn't know what to do. [MA] Students are able to realise something's off, but not exactly able to identify what the implications of that might be. So, some examples of discussions we've heard are students being uncomfortable with the reasons for removing certain outliers and feeling like it was justified after-the-fact. [BC] They're told to go back and find something. [MA] There are students who hide p values from their supervisors. Instead of reporting that a p value was like .07, they'd just say it's bigger than .05 because they know if they say that it's .07 then their supervisor will try to find a way to get them to make it less than .05. Many students are uncomfortable with what's going on. [BC] Lot's of them are the clinical students, so they just want to get out. [MA] We're mostly interested in the little things they can do to make a difference, rather than radically challenge their supervisors. We're interested in your thoughts. Our first question today is "what is open science?"**

[BN] I think open science is transparency not just of the outcomes of research, not just of the content, the data, the materials, the products, the code, but also the process of research. How do we get from the initial idea, the questions that we want to ask, to the claims that we make in our conclusions? So that includes the decisions about design, the decisions about what in advance we know we're trying to do with the project, and then the decisions about how we analyse and interpret that data. Whether it's stuff planned beforehand, or changes after-the-fact. So, the 'openness' part is a mechanism of both credibility of claims so you can evaluate what I did and the basis for making the claims that I made and for an accountability step, so I know that you could look at how I got to the claims that I made. So, when I'm making those choices about analysing the data, what contingent decisions did I make? How might I treat exclusion rules properly in different contexts. I know, at least, that you will look at that and say 'Is that a reasonable choice? Would I have made the same decisions under that same circumstance?'

**[MA] You're name has basically become synonymous with the idea of open science and reproducibility. Is there where you expected to be in your career?**

[BN] It's hard to say that I expected to be where I am, because lots of things just sort of happened and they were opportunities I could not have predicted. But, the things that I'm working on are the things that I've cared about since getting into graduate school. Just the opportunities that have presented themselves are much larger than what I expected to be able to work on. Entering grad school, the most interesting experience for me and getting interested in open science and reproducibility was taking a graduate methods class ??Alan Kazdin?? (4:10) where we read all these papers from the 1960s and 70s from Jacob Cohen, Robert Rosenthal, Tony Greenwald (???? 4:10), Paul Meehl, where they're describing all these problems with the literature. They said low powered research, null results are being ignored, a big [file drawer problem](https://psycnet.apa.org/record/1979-27602-001), we likely have more false positives in the literature than we should, or that we recognise. The bizarre part was saying, we're reading these, they're from the 1960s, they're describing all these problems, they lay out the solutions: replication, preregistration, full transparency. I'm in grad school in the late 90s, why are we talking about this still? They figured it out. What's going on? So the fascinating part to me was this recognition that we can---not just in science, in general, but particularly in science---we can know what the problem is, we can know what the solution is, and still not fix it. And that's amazing, right? That's amazing. Like we all say "Oh yeah, that sucks. Alright, keep writing", and then just sort of move on. And so, the implications for me was saying let's figure out how to address these problems in the part of the world that I can control. In the research that I do, how can we address power? Let's start finding ways to do high-powered research. So, we started collecting data online since 1998, and solve the power problem for the kind of work that I do. Then when I got a faculty position, we started sharing our papers online, when they were drafts, and say "lets make this stuff more available, so people know what we're doing". When [Dataverse](https://dataverse.org/) was created in 2005, we started posting out data on Dataverse. Some people downloaded it, and some people didn't. Just sort of saying "This is important and I can do this, so let's do it for our own work". But then there's just interesting opportunities that present themselves along the way, that say, "oh, well, you could build a tool to make it easier for other people to do this". (6:30)
